{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed English without dataframe (not found) and saved markdown to ../complete_prompts/en_complete.md\n",
      "Processed German without dataframe (not found) and saved markdown to ../complete_prompts/de_complete.md\n",
      "Processed French with dataframe and saved to ../complete_prompts/fr_complete.md\n",
      "Processed Spanish without dataframe (not found) and saved markdown to ../complete_prompts/es_complete.md\n",
      "Processed Italian with dataframe and saved to ../complete_prompts/it_complete.md\n",
      "Processed Portuguese without dataframe (not found) and saved markdown to ../complete_prompts/pt_complete.md\n",
      "Processed Japanese with dataframe and saved to ../complete_prompts/jn_complete.md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from config import column_names\n",
    "\n",
    "\n",
    "# # Load language codes\n",
    "# json_file_path = 'language_codes.json'\n",
    "\n",
    "# with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "#     language_codes = json.load(file)\n",
    "\n",
    "language_codes = {\n",
    "    \"en\": \"English\",\n",
    "    \"de\": \"German\",\n",
    "    \"fr\": \"French\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"jn\": \"Japanese\",\n",
    "}\n",
    "\n",
    "def build_markdown(language):\n",
    "    base_url = \"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/main/prompts/\"\n",
    "    general_url = f\"{base_url}_general_prompt.md\"\n",
    "    examples_url = f\"{base_url}{language}_examples.md\"\n",
    "    \n",
    "    # Load general markdown\n",
    "    response = requests.get(general_url)\n",
    "    general_content = response.text\n",
    "    \n",
    "    # Load examples markdown\n",
    "    response = requests.get(examples_url)\n",
    "    examples_content = response.text\n",
    "    \n",
    "    # Reconstruct the markdown content\n",
    "    final_content = general_content + \"\\n\\n\" + examples_content\n",
    "    \n",
    "    return final_content\n",
    "    # replace 'target_language' and 'native_language' with the actual language\n",
    "\n",
    "\n",
    "for language_key in language_codes:\n",
    "    language_full_name = language_codes.get(language_key, \"Unknown Language\")\n",
    "    markdown = build_markdown(language_key)\n",
    "    markdown = markdown.replace(\"target_language\", language_full_name)\n",
    "\n",
    "    output_file = f\"../complete_prompts/{language_key}_complete.md\"\n",
    "    \n",
    "    # Check if language-specific dataframe exists\n",
    "    df_path = Path(f'../test_dataframes/{language_key}_items/items.csv')\n",
    "    if df_path.exists():\n",
    "        # Load dataframe from CSV\n",
    "        df = pd.read_csv(df_path, delimiter='\\t', encoding='utf-8')\n",
    "        df.columns = column_names\n",
    "        df = df[[\n",
    "            \"Word\",\n",
    "            \"Context\",\n",
    "            \"Context machine translation\",\n",
    "            \"Context human translation\",\n",
    "        ]]\n",
    "        \n",
    "        with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "            file.write(markdown + '\\n\\nThis is the table with the word sentence pairs:\\n\\n' +\n",
    "                       df.to_csv(sep='\\t', encoding='utf-8', index=False))\n",
    "        \n",
    "        print(f\"Processed {language_full_name} with dataframe and saved to {output_file}\")\n",
    "    else:\n",
    "        # Save only the markdown content if no dataframe exists\n",
    "        with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "            file.write(markdown)\n",
    "        \n",
    "        print(f\"Processed {language_full_name} without dataframe (not found) and saved markdown to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_anki_deck\u001b[39m(df: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m genanki\u001b[38;5;241m.\u001b[39mPackage:\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates an Anki deck from a DataFrame.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        genanki.Package: The generated Anki package.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemplates/anki_card.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m content_file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import genanki\n",
    "from config import fields_config\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def generate_anki_deck(df: pd.DataFrame) -> genanki.Package:\n",
    "    \"\"\"Generates an Anki deck from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing card data.\n",
    "\n",
    "    Returns:\n",
    "        genanki.Package: The generated Anki package.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('templates/anki_card.html', 'r', encoding='utf-8') as content_file:\n",
    "        content = content_file.read()\n",
    "\n",
    "    # Splitting HTML content\n",
    "    html_sections = content.split('<!-- html -->')\n",
    "\n",
    "    # Assigning sections to qfmt, afmt, and css\n",
    "    qfmt_html = html_sections[1]\n",
    "    afmt_html = html_sections[2]\n",
    "\n",
    "    with open('static/css/anki_card.css', 'r', encoding='utf-8') as content_file:\n",
    "        css_code = content_file.read()\n",
    "\n",
    "    # Ensure all columns are strings\n",
    "    df = df.astype(str)\n",
    "\n",
    "    # Define the Anki model\n",
    "    model_id = 1607392319\n",
    "    model = genanki.Model(\n",
    "        model_id,\n",
    "        'Language Learning with Netflix Model',\n",
    "        fields = fields_config[\"fields\"],\n",
    "        templates=[\n",
    "            {\n",
    "                'name': 'Card 1',\n",
    "                'qfmt': qfmt_html,\n",
    "                'afmt': afmt_html,\n",
    "            },\n",
    "        ],\n",
    "        css=css_code\n",
    "    )\n",
    "\n",
    "    # Create an Anki deck\n",
    "    deck_id = model_id + 1  # Ensure deck_id is different from model_id\n",
    "    deck = genanki.Deck(deck_id, 'lln_anki_deck')\n",
    "\n",
    "    # Add cards to the deck\n",
    "    for index, row in df.iterrows():\n",
    "        my_note = genanki.Note(\n",
    "            model=model,\n",
    "            fields=[row['ID'], row['cloze'], row['hint'], row['definition'], row['notes'], row['image'], row['audio']],\n",
    "        )\n",
    "        deck.add_note(my_note)\n",
    "\n",
    "    apkg_package = genanki.Package(deck)\n",
    "    return apkg_package\n",
    "\n",
    "@log_io\n",
    "def export_df(df: pd.DataFrame, package: genanki.Package, native_language: str, output_file_path: str, encoding: str = 'utf-8') -> Tuple[str, str]:\n",
    "    \"\"\"Exports an Anki package and a cleaned DataFrame to CSV.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to export.\n",
    "        package (genanki.Package): The Anki package to save.\n",
    "        native_language (str): The native language of the data.\n",
    "        output_file_path (str): The path to save the files.\n",
    "        encoding (str, optional): The encoding for the CSV file. Defaults to 'utf-8'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing the paths to the exported Anki package and CSV file.\n",
    "    \"\"\"\n",
    "    current_time = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "    package_path = os.path.join(output_file_path, f'{native_language}_LLN_{current_time}.apkg')\n",
    "    package.write_to_file(package_path)\n",
    "\n",
    "    csv_file_path = os.path.join(output_file_path, f'{native_language}_LLN_{current_time}.csv')\n",
    "    df.to_csv(csv_file_path, index=False, sep='\\t', encoding=encoding)\n",
    "\n",
    "    return package_path, csv_file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
